{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202225c9-9127-40b5-8d92-f3aac959ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb72fe-19d6-484c-9ac4-2e341f1836c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate ground truth and drift measurement\n",
    "mean = 4.0\n",
    "std_dev = 0.2\n",
    "length = 1000\n",
    "drift_free_array = torch.normal(mean=mean, std=std_dev, size=(length,))\n",
    "\n",
    "\n",
    "def create_sinusoidal_drift(length, amplitude=0.5, frequency=1.0):\n",
    "    t = torch.arange(length)\n",
    "    drift = amplitude * torch.sin(2 * np.pi * frequency * t / length)\n",
    "    return drift\n",
    "\n",
    "amplitude = 0.5\n",
    "frequency = 1.0\n",
    "\n",
    "num_measurements = 1000\n",
    "patch_length = 20\n",
    "\n",
    "\n",
    "drifted_measurements = []\n",
    "ground_truth = []\n",
    "\n",
    "\n",
    "for _ in range(num_measurements):\n",
    "    start_idx = torch.randint(0, length - patch_length + 1, (1,)).item()\n",
    "    \n",
    "    patch = drift_free_array[start_idx:start_idx + patch_length]\n",
    "    \n",
    "    drift = create_sinusoidal_drift(patch_length, amplitude, frequency)\n",
    "    \n",
    "    drifted_patch = patch + drift\n",
    "    \n",
    "    drifted_measurements.append(drifted_patch)\n",
    "    ground_truth.append(patch)\n",
    "\n",
    "drifted_measurements_tensor = torch.stack(drifted_measurements)\n",
    "ground_truth_tensor = torch.stack(ground_truth)\n",
    "\n",
    "dataset = TensorDataset(drifted_measurements_tensor, ground_truth_tensor)\n",
    "\n",
    "train_ratio = 0.8 \n",
    "\n",
    "train_length = int(train_ratio * num_measurements)\n",
    "val_length = num_measurements - train_length\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_length, val_length])\n",
    "\n",
    "batch_size = 32 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3766419-bb53-4d0d-884b-aecf49485629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder_drifted = nn.Sequential(\n",
    "            nn.Linear(5, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 20)\n",
    "        )\n",
    "        self.decoder_drift_free = nn.Sequential(\n",
    "            nn.Linear(5, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 20)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        drifted_output = self.decoder_drifted(latent)\n",
    "        drift_free_output = self.decoder_drift_free(latent)\n",
    "        return drifted_output, drift_free_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51154258-6260-43f4-8fef-8100bba09a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163a307-38ac-47be-87bd-8b5cc25b94eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 33.09877899169922, Validation Loss: 32.589642660958425\n",
      "Epoch 2/100, Train Loss: 31.77648338317871, Validation Loss: 30.397002356392996\n",
      "Epoch 3/100, Train Loss: 27.315767669677733, Validation Loss: 22.591331481933594\n",
      "Epoch 4/100, Train Loss: 16.604866828918457, Validation Loss: 10.954116140093122\n",
      "Epoch 5/100, Train Loss: 7.925512065887451, Validation Loss: 5.232173102242606\n",
      "Epoch 6/100, Train Loss: 3.6235390377044676, Validation Loss: 2.264804090772356\n",
      "Epoch 7/100, Train Loss: 1.4687604093551636, Validation Loss: 0.8512161459241595\n",
      "Epoch 8/100, Train Loss: 0.5365648078918457, Validation Loss: 0.3191154769488743\n",
      "Epoch 9/100, Train Loss: 0.21817026495933534, Validation Loss: 0.15420228668621608\n",
      "Epoch 10/100, Train Loss: 0.12135739505290985, Validation Loss: 0.10280043312481471\n",
      "Epoch 11/100, Train Loss: 0.09042074978351593, Validation Loss: 0.08505382069519588\n",
      "Epoch 12/100, Train Loss: 0.08014327824115754, Validation Loss: 0.07920873271567481\n",
      "Epoch 13/100, Train Loss: 0.07685134738683701, Validation Loss: 0.07725478495870318\n",
      "Epoch 14/100, Train Loss: 0.07587547659873962, Validation Loss: 0.07665083450930459\n",
      "Epoch 15/100, Train Loss: 0.0755908015370369, Validation Loss: 0.07646459660359792\n",
      "Epoch 16/100, Train Loss: 0.07552199572324753, Validation Loss: 0.07635913363524846\n",
      "Epoch 17/100, Train Loss: 0.07556499928236007, Validation Loss: 0.0763215252331325\n",
      "Epoch 18/100, Train Loss: 0.0755329218506813, Validation Loss: 0.07644060573407582\n",
      "Epoch 19/100, Train Loss: 0.07551834821701049, Validation Loss: 0.07633666800601142\n",
      "Epoch 20/100, Train Loss: 0.07552507758140564, Validation Loss: 0.07639354360955102\n",
      "Epoch 21/100, Train Loss: 0.07553472340106965, Validation Loss: 0.07635375963790077\n",
      "Epoch 22/100, Train Loss: 0.0755030283331871, Validation Loss: 0.07636297600609916\n",
      "Epoch 23/100, Train Loss: 0.07554068952798844, Validation Loss: 0.07635607357536044\n",
      "Epoch 24/100, Train Loss: 0.0755498531460762, Validation Loss: 0.0765901261142322\n",
      "Epoch 25/100, Train Loss: 0.07556377917528152, Validation Loss: 0.07638831117323466\n",
      "Epoch 26/100, Train Loss: 0.07553388118743896, Validation Loss: 0.07635148401771273\n",
      "Epoch 27/100, Train Loss: 0.07551923751831055, Validation Loss: 0.07634057317461286\n",
      "Epoch 28/100, Train Loss: 0.07554319173097611, Validation Loss: 0.07637943753174373\n",
      "Epoch 29/100, Train Loss: 0.07556987226009369, Validation Loss: 0.07649934611150197\n",
      "Epoch 30/100, Train Loss: 0.07561419755220414, Validation Loss: 0.07634701154061727\n",
      "Epoch 31/100, Train Loss: 0.07561848789453507, Validation Loss: 0.07635339775255748\n",
      "Epoch 32/100, Train Loss: 0.07564828217029572, Validation Loss: 0.07649058848619461\n",
      "Epoch 33/100, Train Loss: 0.07561548858880997, Validation Loss: 0.07641989524875369\n",
      "Epoch 34/100, Train Loss: 0.07560669928789139, Validation Loss: 0.07650774823767799\n",
      "Epoch 35/100, Train Loss: 0.07558364719152451, Validation Loss: 0.07622827695948738\n",
      "Epoch 36/100, Train Loss: 0.07560809314250946, Validation Loss: 0.07636598816939763\n",
      "Epoch 37/100, Train Loss: 0.07561660557985306, Validation Loss: 0.07654968010527748\n",
      "Epoch 38/100, Train Loss: 0.0755884137749672, Validation Loss: 0.07637561219079154\n",
      "Epoch 39/100, Train Loss: 0.07558627396821976, Validation Loss: 0.07640217883246285\n",
      "Epoch 40/100, Train Loss: 0.07559767812490463, Validation Loss: 0.07641117594071797\n",
      "Epoch 41/100, Train Loss: 0.07555817693471908, Validation Loss: 0.07640410427536283\n",
      "Epoch 42/100, Train Loss: 0.0756034779548645, Validation Loss: 0.0764373147061893\n",
      "Epoch 43/100, Train Loss: 0.07562119662761688, Validation Loss: 0.07654432952404022\n",
      "Epoch 44/100, Train Loss: 0.07561798751354218, Validation Loss: 0.07633635933910098\n",
      "Epoch 45/100, Train Loss: 0.07555574774742127, Validation Loss: 0.07635639607906342\n",
      "Epoch 46/100, Train Loss: 0.07562385648488998, Validation Loss: 0.07665203405278069\n",
      "Epoch 47/100, Train Loss: 0.07564063727855683, Validation Loss: 0.07641440310648509\n",
      "Epoch 48/100, Train Loss: 0.07563601553440094, Validation Loss: 0.07639990959848676\n",
      "Epoch 49/100, Train Loss: 0.07562387079000472, Validation Loss: 0.07641870102712087\n",
      "Epoch 50/100, Train Loss: 0.07567543268203736, Validation Loss: 0.07664542645215988\n",
      "Epoch 51/100, Train Loss: 0.07562321484088898, Validation Loss: 0.07638554487909589\n",
      "Epoch 52/100, Train Loss: 0.07558057129383088, Validation Loss: 0.07648476106779915\n",
      "Epoch 53/100, Train Loss: 0.07559478372335433, Validation Loss: 0.07647335210016795\n",
      "Epoch 54/100, Train Loss: 0.07562554657459258, Validation Loss: 0.07646571738379342\n",
      "Epoch 55/100, Train Loss: 0.07573536485433578, Validation Loss: 0.07630799604313714\n",
      "Epoch 56/100, Train Loss: 0.07565802365541457, Validation Loss: 0.07671243165220533\n",
      "Epoch 57/100, Train Loss: 0.07571078479290008, Validation Loss: 0.07651202167783465\n",
      "Epoch 58/100, Train Loss: 0.07564292818307877, Validation Loss: 0.07650382391044072\n",
      "Epoch 59/100, Train Loss: 0.07575630307197571, Validation Loss: 0.07658302677529198\n",
      "Epoch 60/100, Train Loss: 0.07570653975009918, Validation Loss: 0.07652101452861514\n",
      "Epoch 61/100, Train Loss: 0.07566541135311126, Validation Loss: 0.07666962274483272\n",
      "Epoch 62/100, Train Loss: 0.07567041873931885, Validation Loss: 0.07666295766830444\n",
      "Epoch 63/100, Train Loss: 0.07566622704267502, Validation Loss: 0.0766921266913414\n",
      "Epoch 64/100, Train Loss: 0.07562904804944992, Validation Loss: 0.07651393860578537\n",
      "Epoch 65/100, Train Loss: 0.07561282157897949, Validation Loss: 0.0766541138291359\n",
      "Epoch 66/100, Train Loss: 0.07563285052776336, Validation Loss: 0.07631301879882812\n",
      "Epoch 67/100, Train Loss: 0.07566482484340668, Validation Loss: 0.07629339077642985\n",
      "Epoch 68/100, Train Loss: 0.07561414569616318, Validation Loss: 0.07647709122725896\n",
      "Epoch 69/100, Train Loss: 0.07565508991479873, Validation Loss: 0.07652911756719862\n",
      "Epoch 70/100, Train Loss: 0.07564371824264526, Validation Loss: 0.07634503287928444\n",
      "Epoch 71/100, Train Loss: 0.07567363530397415, Validation Loss: 0.07600276810782296\n",
      "Epoch 72/100, Train Loss: 0.07569023430347442, Validation Loss: 0.0767679853098733\n",
      "Epoch 73/100, Train Loss: 0.07563234955072402, Validation Loss: 0.07633164844342641\n",
      "Epoch 74/100, Train Loss: 0.07577228248119354, Validation Loss: 0.07638129804815565\n",
      "Epoch 75/100, Train Loss: 0.07563359409570694, Validation Loss: 0.07653926206486565\n",
      "Epoch 76/100, Train Loss: 0.07561722815036774, Validation Loss: 0.07669918877737862\n",
      "Epoch 77/100, Train Loss: 0.07571667551994324, Validation Loss: 0.07665348052978516\n",
      "Epoch 78/100, Train Loss: 0.07570200830698014, Validation Loss: 0.0762214937380382\n",
      "Epoch 79/100, Train Loss: 0.07567246496677399, Validation Loss: 0.0761903662766729\n",
      "Epoch 80/100, Train Loss: 0.07564571231603623, Validation Loss: 0.0763015363897596\n",
      "Epoch 81/100, Train Loss: 0.07569097727537155, Validation Loss: 0.07658774512154716\n",
      "Epoch 82/100, Train Loss: 0.07564278572797775, Validation Loss: 0.07636834468160357\n",
      "Epoch 83/100, Train Loss: 0.07577024519443512, Validation Loss: 0.07664712837764195\n",
      "Epoch 84/100, Train Loss: 0.075690256357193, Validation Loss: 0.07637288314955575\n",
      "Epoch 85/100, Train Loss: 0.07562976419925689, Validation Loss: 0.07656024502856391\n",
      "Epoch 86/100, Train Loss: 0.07579610332846641, Validation Loss: 0.07675332576036453\n",
      "Epoch 87/100, Train Loss: 0.07579948365688324, Validation Loss: 0.07638671568461827\n",
      "Epoch 88/100, Train Loss: 0.07570964187383651, Validation Loss: 0.07638677741800036\n",
      "Epoch 89/100, Train Loss: 0.0757106328010559, Validation Loss: 0.07687356855188097\n",
      "Epoch 90/100, Train Loss: 0.07577290683984757, Validation Loss: 0.07622530311346054\n",
      "Epoch 91/100, Train Loss: 0.07569053202867508, Validation Loss: 0.07647224515676498\n",
      "Epoch 92/100, Train Loss: 0.07566375821828843, Validation Loss: 0.07633912563323975\n",
      "Epoch 93/100, Train Loss: 0.07569943606853485, Validation Loss: 0.07656852900981903\n",
      "Epoch 94/100, Train Loss: 0.07570336908102035, Validation Loss: 0.07639534558568682\n",
      "Epoch 95/100, Train Loss: 0.07572299987077713, Validation Loss: 0.07626714344535555\n",
      "Epoch 96/100, Train Loss: 0.07570171117782593, Validation Loss: 0.07644729209797722\n",
      "Epoch 97/100, Train Loss: 0.07562666445970535, Validation Loss: 0.0762279640351023\n",
      "Epoch 98/100, Train Loss: 0.07566037207841873, Validation Loss: 0.07641514497143882\n",
      "Epoch 99/100, Train Loss: 0.0757402953505516, Validation Loss: 0.07665956126792091\n",
      "Epoch 100/100, Train Loss: 0.07562393456697464, Validation Loss: 0.07675807710204806\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    \n",
    "    autoencoder.train()\n",
    "    for drifted_batch, ground_truth_batch in train_loader:\n",
    "        drifted_output, drift_free_output = autoencoder(drifted_batch)\n",
    "\n",
    "        drifted_loss = mse_loss(drifted_output, drifted_batch)\n",
    "        drift_free_loss = mse_loss(drift_free_output, ground_truth_batch)\n",
    "\n",
    "        loss = drifted_loss + drift_free_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for drifted_batch, ground_truth_batch in val_loader:\n",
    "            drifted_output, drift_free_output = autoencoder(drifted_batch)\n",
    "\n",
    "            drifted_loss = mse_loss(drifted_output, drifted_batch)\n",
    "            drift_free_loss = mse_loss(drift_free_output, ground_truth_batch)\n",
    "\n",
    "            loss = drifted_loss + drift_free_loss\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df5186-d3e1-469e-817a-c61a166bda3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
