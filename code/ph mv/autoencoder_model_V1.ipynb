{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c8c0f0f-2da3-41c5-830f-abd44db3b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cf34d0b-df10-43ef-8044-98fda60019f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "at200 = pd.read_csv(\"../data/csv_files/at200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9830e495-c1d6-4c8e-8218-dea6e494555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def preprocess(df):\n",
    "    df2 = df.dropna()\n",
    "    le = LabelEncoder()\n",
    "    df2.iloc[:, 7] = le.fit_transform(df2.iloc[:, 7]) #encode the station id\n",
    "    df2['Date Time'] = df2['Date Time'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f').timestamp() * 1000) # encode date time\n",
    "    df2 = df2.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    #normalize\n",
    "    scaler = StandardScaler()\n",
    "    df2 = pd.DataFrame(scaler.fit_transform(df2), columns=df2.columns)\n",
    "\n",
    "    tensor = df2.values\n",
    "    tensor = torch.tensor(tensor, dtype = torch.float32)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d90324b3-fdfa-4eb9-8a45-87edd176e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/w8y1hffs2j92q_glhgdblry80000gn/T/ipykernel_13136/4040558709.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Date Time'] = df2['Date Time'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f').timestamp() * 1000) # encode date time\n"
     ]
    }
   ],
   "source": [
    "preprocess_at200 = preprocess(at200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcd3d8b6-4ff4-42a1-931b-ea8cadd7eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 6),  # Input layer with 10 hidden units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 3),  # Hidden layer with 16 hidden units\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, input_dim)\n",
    "\n",
    "         # nn.Sigmoi# Output layer with same dimension as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "531b84a7-5f91-4677-8340-e94175b7a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  avg Loss: 0.35664396573354296\n",
      "Epoch: 1  avg Loss: 0.09019972608903923\n",
      "Epoch: 2  avg Loss: 0.054030212695238715\n",
      "Epoch: 3  avg Loss: 0.048410672435009294\n",
      "Epoch: 4  avg Loss: 0.04648895971754099\n",
      "Epoch: 5  avg Loss: 0.044298847212233886\n",
      "Epoch: 6  avg Loss: 0.0413095926402478\n",
      "Epoch: 7  avg Loss: 0.037704866757947625\n",
      "Epoch: 8  avg Loss: 0.03442351232856472\n",
      "Epoch: 9  avg Loss: 0.03222705874758306\n",
      "Epoch: 10  avg Loss: 0.030847508323638927\n",
      "Epoch: 11  avg Loss: 0.030156010760597637\n",
      "Epoch: 12  avg Loss: 0.02968275011280971\n",
      "Epoch: 13  avg Loss: 0.02917775687049417\n",
      "Epoch: 14  avg Loss: 0.02882609849995943\n",
      "Epoch: 15  avg Loss: 0.028508719595865754\n",
      "Epoch: 16  avg Loss: 0.028014582758763097\n",
      "Epoch: 17  avg Loss: 0.027446434053543897\n",
      "Epoch: 18  avg Loss: 0.026565194688485512\n",
      "Epoch: 19  avg Loss: 0.02543328533376025\n",
      "Epoch: 20  avg Loss: 0.024126345855312988\n",
      "Epoch: 21  avg Loss: 0.023087069957672623\n",
      "Epoch: 22  avg Loss: 0.022440037185715373\n",
      "Epoch: 23  avg Loss: 0.02211932487599426\n",
      "Epoch: 24  avg Loss: 0.02189854881710463\n",
      "Epoch: 25  avg Loss: 0.02171426172040156\n",
      "Epoch: 26  avg Loss: 0.021539264299388548\n",
      "Epoch: 27  avg Loss: 0.021425920119225738\n",
      "Epoch: 28  avg Loss: 0.02136141389288497\n",
      "Epoch: 29  avg Loss: 0.021301144600529134\n",
      "Epoch: 30  avg Loss: 0.021233994144747352\n",
      "Epoch: 31  avg Loss: 0.02124770537605654\n",
      "Epoch: 32  avg Loss: 0.021139755029611646\n",
      "Epoch: 33  avg Loss: 0.021102147163638115\n",
      "Epoch: 34  avg Loss: 0.021122134270485267\n",
      "Epoch: 35  avg Loss: 0.021069712982216424\n",
      "Epoch: 36  avg Loss: 0.021032563300502308\n",
      "Epoch: 37  avg Loss: 0.020998386323623275\n",
      "Epoch: 38  avg Loss: 0.02100599863477671\n",
      "Epoch: 39  avg Loss: 0.021001798194909292\n",
      "Epoch: 40  avg Loss: 0.020971037301992426\n",
      "Epoch: 41  avg Loss: 0.020959636826821282\n",
      "Epoch: 42  avg Loss: 0.020926051652543826\n",
      "Epoch: 43  avg Loss: 0.02094901639381279\n",
      "Epoch: 44  avg Loss: 0.020900302688749237\n",
      "Epoch: 45  avg Loss: 0.02093048811971416\n",
      "Epoch: 46  avg Loss: 0.020920067017128473\n",
      "Epoch: 47  avg Loss: 0.020859974359323984\n",
      "Epoch: 48  avg Loss: 0.020872311032502495\n",
      "Epoch: 49  avg Loss: 0.02088185373449382\n",
      "Epoch: 50  avg Loss: 0.02085661999012285\n",
      "Epoch: 51  avg Loss: 0.020861301408039105\n",
      "Epoch: 52  avg Loss: 0.020790972621598867\n",
      "Epoch: 53  avg Loss: 0.02085788175904465\n",
      "Epoch: 54  avg Loss: 0.020773865477741772\n",
      "Epoch: 55  avg Loss: 0.02081724935090969\n",
      "Epoch: 56  avg Loss: 0.020805887719572216\n",
      "Epoch: 57  avg Loss: 0.020709891122229967\n",
      "Epoch: 58  avg Loss: 0.020701109517952572\n",
      "Epoch: 59  avg Loss: 0.02070516233783948\n",
      "Epoch: 60  avg Loss: 0.020665690455865304\n",
      "Epoch: 61  avg Loss: 0.020656461290325\n",
      "Epoch: 62  avg Loss: 0.02062387874902826\n",
      "Epoch: 63  avg Loss: 0.020632538479766063\n",
      "Epoch: 64  avg Loss: 0.020612603243619222\n",
      "Epoch: 65  avg Loss: 0.020604186373346555\n",
      "Epoch: 66  avg Loss: 0.020568391168395598\n",
      "Epoch: 67  avg Loss: 0.02055539290639216\n",
      "Epoch: 68  avg Loss: 0.020549081259629388\n",
      "Epoch: 69  avg Loss: 0.02057269890084258\n",
      "Epoch: 70  avg Loss: 0.020545214428764752\n",
      "Epoch: 71  avg Loss: 0.02055038414087003\n",
      "Epoch: 72  avg Loss: 0.020563262979486772\n",
      "Epoch: 73  avg Loss: 0.020526786426748475\n",
      "Epoch: 74  avg Loss: 0.020541885161509156\n",
      "Epoch: 75  avg Loss: 0.020505424143722566\n",
      "Epoch: 76  avg Loss: 0.020540276518062592\n",
      "Epoch: 77  avg Loss: 0.02053949562108561\n",
      "Epoch: 78  avg Loss: 0.020518672235053163\n",
      "Epoch: 79  avg Loss: 0.020468012144520847\n",
      "Epoch: 80  avg Loss: 0.020505924620983607\n",
      "Epoch: 81  avg Loss: 0.020512464000359228\n",
      "Epoch: 82  avg Loss: 0.020477553478809245\n",
      "Epoch: 83  avg Loss: 0.020488302451964566\n",
      "Epoch: 84  avg Loss: 0.02048617905758931\n",
      "Epoch: 85  avg Loss: 0.020458608952636168\n",
      "Epoch: 86  avg Loss: 0.02046576756643959\n",
      "Epoch: 87  avg Loss: 0.02044056833326877\n",
      "Epoch: 88  avg Loss: 0.020486731816428266\n",
      "Epoch: 89  avg Loss: 0.02048879194163426\n",
      "Epoch: 90  avg Loss: 0.020455493797007205\n",
      "Epoch: 91  avg Loss: 0.02047545629559164\n",
      "Epoch: 92  avg Loss: 0.02046440601817372\n",
      "Epoch: 93  avg Loss: 0.020453012049036948\n",
      "Epoch: 94  avg Loss: 0.02044256658817845\n",
      "Epoch: 95  avg Loss: 0.020421603469126446\n",
      "Epoch: 96  avg Loss: 0.02042885630173033\n",
      "Epoch: 97  avg Loss: 0.020430418233588047\n",
      "Epoch: 98  avg Loss: 0.0204406790854063\n",
      "Epoch: 99  avg Loss: 0.02038057366080023\n",
      "Autoencoder training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of the Autoencoder\n",
    "batches = DataLoader(preprocess_at200, batch_size = 32, shuffle = True)\n",
    "model = Autoencoder(input_dim=10)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train the autoencoder\n",
    "for epoch in range(100):  \n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in batches:\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        loss = loss_fn(outputs, batch)\n",
    "        \n",
    "        # Backward pass and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    model.eval()\n",
    "    avg_loss = total_loss / len(batches)\n",
    "    print(f\"Epoch: {epoch}  avg Loss: {avg_loss}\")\n",
    "\n",
    "print(\"Autoencoder training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1b63b-8dd0-4844-b4b7-000f96988b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
